


# library manipulation dataset
import pandas as pd
from pandas import concat
from pandas import DataFrame
from pandas import read_csv
from pandas import read_excel

# library manipulation array
import numpy as np
from numpy import concatenate
from numpy import array

# library configuration date and time
import time
from datetime import datetime

# library data visualization
import seaborn as sns
import matplotlib.dates as mdates
from matplotlib import pyplot
from matplotlib import pyplot as plt

# library analysis acf and pacf
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.graphics.tsaplots import plot_acf

# library normalize data with max-min algorithm
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

# library algorithm lstm-rnn with keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import RNN
from keras.layers import LSTM
from keras.layers import GRU
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import TimeDistributed
from keras.layers import Bidirectional
from keras.optimizers import Adam, Adamax, RMSprop, SGD
from keras.layers import LeakyReLU

# Early stoping
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint

# library evaluation model
from math import sqrt
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error


# Set waktu komputasi
start = time.time()


# fix random seed for reproducibility
np.random.seed(42)


# membaca dataset
dataset = read_excel("dataset/dataset.xlsx")


# set index tanggal
dataset = dataset.set_index("tanggal")


dataset.info()


print(dataset.head())





# memilih area studi
df_sumsel = dataset[["hotspot_sumsel", "sst", "soi"]]
df_sumsel.info()


print(df_sumsel.head())


# ensure all data is float
values = df_sumsel.values
values = values.astype('float64')





# normalize features
scaler = MinMaxScaler(feature_range=(-1, 1))
scaled = scaler.fit_transform(values)


np.round(scaled[:5],6)





# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    
    # return value
    return agg


# supervised learning
reframed = series_to_supervised(scaled, 1, 1)


reframed.head()


# drop columns we don't want to predict
reframed.drop(reframed.columns[[4,5]], axis=1, inplace=True)


values = reframed.values


reframed.head()





# split into train and test sets
train_size = int(len(values) * 0.8)
test_size = len(values) - train_size
train, test = values[0:train_size,:], values[train_size:len(values),:]


# split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]





temp_train_X = pd.DataFrame(train_X)
temp_train_y = pd.DataFrame(train_y)


hasil = pd.concat([temp_train_X, temp_train_y], axis=1)
hasil.head()





temp_test_X = pd.DataFrame(test_X)
temp_test_y = pd.DataFrame(test_y)


hasil = pd.concat([temp_test_X, temp_test_y], axis=1)
hasil.head()





# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))


print(train_X.shape, train_y.shape)


print(test_X.shape, test_y.shape)





# design network grid serach
model = Sequential()

# First LSTM layer with Dropout regularisation
model.add(
    LSTM(
        units=10,
        activation='elu',
        input_shape=(train_X.shape[1], train_X.shape[2])
    )
)
model.add(Dropout(0.15))

# The output layer
model.add(Dense(1))

# Compiling model the LSTM-RNN
model.compile(
    optimizer='rmsprop',
    loss='mae',
    metrics=[
        tf.keras.metrics.MeanAbsoluteError(),
        tf.keras.metrics.MeanSquaredError(),
        tf.keras.metrics.RootMeanSquaredError()
    ]
)


# fit network
history = model.fit(train_X, train_y, epochs=2000, batch_size=16,
                    validation_data=(test_X, test_y), use_multiprocessing=True,
                    verbose=0, shuffle=False)


model.summary()


# membuat frame
fig, ax = plt.subplots(figsize = (10,5))

# membuat time series plot
ax.plot(history.history['loss'], color="tab:blue", label="train", linewidth=1.5)
ax.plot(history.history['val_loss'], color="tab:orange", label="test", linewidth=1.5)

# membuat label-label
ax.set_title("Grafik Loss Function", fontsize=14)
ax.legend(loc='upper right')
ax.grid(True)

# menampilkan plot
plt.show()


# 5. make predictions
predictions = model.predict(test_X, verbose=0)
print(predictions[:, 0])





scores = model.evaluate(train_X, train_y)
scores


scores = model.evaluate(test_X, test_y)
scores





mae = mean_absolute_error(test_y, predictions)
print('Test MAE: %.4f' % mae)





mse = mean_squared_error(test_y, predictions)
print('Test MSE: %.4f' % mse)





# calculate RMSE
rmse = sqrt(mean_squared_error(test_y , predictions))
print('Test RMSE: %.4f' % rmse)





mape = mean_absolute_percentage_error(test_y, predictions)
print('Test MAPE: %.4f' % mape)





# smape = sMAPE(np.array(test_y), np.array(predictions))
# print('Test sMAPE: %.4f' % smape)





hasil = np.stack((test_y.reshape(-1),predictions.reshape(-1)),axis=1)
hasil = pd.DataFrame(hasil, columns = ['data_aktual','prediksi'])
hasil.head()


import scipy.stats as sc
r, p = sc.pearsonr(hasil["data_aktual"], hasil["prediksi"])
print("korelasi data akual dengan hasil prediksi" +" {:.4f} ".format(r)+ "dengan signifikansi" +" {:.4f} ".format(p))





# Set akhir waktu komputasi 
end = time.time()


# Proses menghitung waktu komputasi
hours, rem = divmod(end-start, 3600)
minutes, seconds = divmod(rem, 60)


# Hasil waktu komputasi
print("{:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds))





# generate urutan data sesuai panjang datanya
x = pd.date_range(start="2017-01-01", periods=len(test_y), freq='MS')

# membuat frame
fig, ax = plt.subplots(figsize = (8,4))

# membuat time series plot
ax.plot(x, test_y, color="tab:blue", label="data aktual", linewidth=2.5)
ax.plot(x, predictions, color="tab:red", label="hasil prediksi", linewidth=2.5)

# membuat label-label
ax.xaxis.set_major_locator(mdates.YearLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
ax.legend(loc='upper right')
ax.grid(True)

# menampilkan plot
plt.show()





df_sumsel = np.array(dataset["hotspot_sumsel"])


scaler = MinMaxScaler(feature_range=(-1,1))
df_sumsel = scaler.fit_transform(df_sumsel.reshape(-1,1))


# inverse value test predictions
testPredictions = scaler.inverse_transform(predictions)
testActual = scaler.inverse_transform(np.array(test_y).reshape(-1, 1))


# generate urutan data sesuai panjang datanya
x = pd.date_range(start="2017-01-01", periods=len(test_y), freq='MS')

# membuat frame
fig, ax = plt.subplots(figsize = (8,4))

# membuat time series plot
ax.plot(x, testActual, color="tab:blue", label="Actual data", linewidth=2.5)
ax.plot(x, testPredictions, color="tab:red", label="Prediction results", linewidth=2.5)

# membuat label-label
ax.xaxis.set_major_locator(mdates.YearLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
ax.legend(loc="best")
ax.grid(True)

# menampilkan plot
plt.show()


#  shift test predictions for plotting
testPredictionsPlot = np.empty_like(df_sumsel)
testPredictionsPlot[:, :] = np.nan
testPredictionsPlot[(len(dataset) - testPredictions.shape[0]):len(dataset), :] = testPredictions


# membuat frame
fig, ax = plt.subplots(figsize = (8,4))

# membuat time series plot
ax.plot(dataset.index.values, scaler.inverse_transform(df_sumsel), color="tab:blue", label="actual data", linewidth=2)
ax.plot(dataset.index.values, testPredictionsPlot, color="tab:red", label="predictions data", linewidth=2)

# membuat label-label
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))
ax.legend(loc="best")
ax.grid(True)

# menampilkan plot
plt.show()





import xgboost as xgb


# Calculate residuals (difference between LSTM predictions and actual values)
residuals = test_y - predictions[:, 0]


# Train XGBoost model on residuals
xgb_model = xgb.XGBRegressor()
xgb_model.fit(np.vstack(test_X), residuals)


# Predict residuals with XGBoost model
xgb_predictions = xgb_model.predict(np.vstack(test_X))


# Combine LSTM and XGBoost predictions
boosted_predictions = predictions[:, 0] + xgb_predictions


# Use boosted_predictions as the final predictions
print(boosted_predictions)





mae = mean_absolute_error(test_y, boosted_predictions)
print('Test MAE : %.4f' % mae)


mse = mean_squared_error(test_y, boosted_predictions)
print('Test MSE: %.4f' % mse)


# calculate RMSE
rmse = sqrt(mse)
print('Test RMSE: %.4f' % rmse)


mape = mean_absolute_percentage_error(test_y, boosted_predictions)
print('Test MAPE: %.4f' % mape)


hasil = np.stack((test_y, boosted_predictions), axis=1)
hasil = pd.DataFrame(hasil, columns = ['data_aktual','prediksi'])
hasil.head()


import scipy.stats as sc
r, p = sc.pearsonr(hasil["data_aktual"], hasil["prediksi"])
print("korelasi data akual dengan hasil prediksi" +" {:.4f} ".format(r)+ "dengan signifikansi" +" {:.4f} ".format(p))





df_sumsel = np.array(dataset["hotspot_sumsel"])


scaler = MinMaxScaler(feature_range=(-1,1))
df_sumsel = scaler.fit_transform(df_sumsel.reshape(-1,1))


# inverse value test predictions
testPredictions = scaler.inverse_transform(boosted_predictions.reshape(-1, 1))
testActual = scaler.inverse_transform(np.array(test_y).reshape(-1, 1))


# generate urutan data sesuai panjang datanya
x = pd.date_range(start="2017-01-01", periods=len(test_y), freq='MS')

# membuat frame
fig, ax = plt.subplots(figsize = (10,5))

# membuat time series plot
ax.plot(x, testActual, color="tab:blue", label="Actual data", linewidth=2.5)
ax.plot(x, testPredictions, color="tab:red", label="Prediction results", linewidth=2.5)

# membuat label-label
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))
ax.legend(loc="best")
ax.grid(True)

# menampilkan plot
plt.show()


#  shift test predictions for plotting
testPredictionsPlot = np.empty_like(df_sumsel)
testPredictionsPlot[:, :] = np.nan
testPredictionsPlot[(len(dataset) - testPredictions.shape[0]):len(dataset), :] = testPredictions


# membuat frame
fig, ax = plt.subplots(figsize = (10,5))

# membuat time series plot
ax.plot(dataset.index.values, scaler.inverse_transform(df_sumsel), color="tab:blue", label="actual data", linewidth=2)
ax.plot(dataset.index.values, testPredictionsPlot, color="tab:red", label="predictions data", linewidth=2)

# membuat label-label
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))
ax.legend(loc="best")
ax.grid(True)

# menampilkan plot
plt.show()



